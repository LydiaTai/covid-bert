{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CT-BERT - Huggingface (GPU training)",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LydiaTai/covid-bert/blob/main/CT_BERT_Huggingface_(GPU_training).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3gcwjDTd4oY"
      },
      "source": [
        "<img align=\"right\" width=\"450px\" src=\"https://github.com/digitalepidemiologylab/covid-twitter-bert/raw/master/images/COVID-Twitter-BERT-medium.png\">\n",
        "\n",
        "# Finetuning COVID-Twitter-BERT using Huggingface\n",
        "In this notebook we will finetune CT-BERT for sentiment classification using the transformer library by Huggingface.\n",
        "\n",
        "Learn more about this library [here](https://huggingface.co/transformers/).\n",
        "\n",
        "## Before proceeding\n",
        "Create a copy of this notebook by going to \"File - Save a Copy in Drive\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVsmxmSberX2"
      },
      "source": [
        "# Install transformers and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_k-GwPM9JXP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "80a7a970-336e-409b-e7db-da5df62c8b92"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 778kB 5.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 15.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 40.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=d717d2618d9a205fbddae8ca2ee7be9e7110728eadfbd8b187b3767fe071ad88\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvvPnOFQH2pR"
      },
      "source": [
        "from transformers import (\n",
        "   AutoConfig,\n",
        "   AutoTokenizer,\n",
        "   TFAutoModelForSequenceClassification,\n",
        "   glue_convert_examples_to_features\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrDpLTI7Jt5p"
      },
      "source": [
        "# Choose a Model from the Huggingface Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyLIi4bvImJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74037f0-6adc-4869-d776-bd86c3c03109"
      },
      "source": [
        "# Choose model\n",
        "# @markdown >The default model is <i><b>COVID-Twitter-BERT</b></i>. You can however choose <i><b>BERT Base</i></b> or <i><b>BERT Large</i></b> to compare these models to the <i><b>COVID-Twitter-BERT</i></b>. All these three models will be initiated with a random classification layer. If you go directly to the Predict-cell after having compiled the model, you will see that it still runs the predition. However the output will be random. The training steps below will finetune this for the specific task. <br /><br />\n",
        "model_name = 'digitalepidemiologylab/covid-twitter-bert' #@param [\"digitalepidemiologylab/covid-twitter-bert\", \"bert-large-uncased\", \"bert-base-uncased\"]\n",
        "\n",
        "# Initialise tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdiZDL69JpNC"
      },
      "source": [
        "# Download the SST-2 Dataset and Prepare for Finetuning\n",
        "You can skip this step if you are using the already finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11kI0lhIJoxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780e3cc6-0601-4936-8726-e82f57b04fde"
      },
      "source": [
        "# Parameters\n",
        "#@markdown >Batch size and sequence length needs to be set to prepare the data. The size of the batches depends on available memory. For Colab GPU limit batch size to 8 and sequence length to 96. By reducing the length of the input (max_seq_length) you can also increase the batch size. For a dataset like SST-2 with lots of short sentences. this will likely benefit training.\n",
        "max_seq_length = 96 #@param {type: \"integer\"}\n",
        "train_batch_size =  8#@param {type: \"integer\"}\n",
        "eval_batch_size = 8 #@param {type: \"integer\"}\n",
        "\n",
        "#@markdown >The Glue dataset has around 62000 examples, and we really do not need them all for training a decent model. To cut down training time, please reduse this to only a percentage of the entire set.\n",
        "use_percentage_of_data = 10\n",
        "\n",
        "# get dataset sizes\n",
        "glue_builder = tfds.builder('glue/sst2')  # æ³¨æ„è¿™é‡Œæ”¹ä¸ºsst2\n",
        "glue_builder.download_and_prepare() # This line was moved up\n",
        "num_train_examples = glue_builder.info.splits['train'].num_examples\n",
        "num_dev_examples = glue_builder.info.splits['validation'].num_examples\n",
        "num_labels = glue_builder.info.features['label'].num_classes\n",
        "\n",
        "# download datasets\n",
        "glue_builder.download_and_prepare()\n",
        "train_data = glue_builder.as_dataset(split='train')\n",
        "dev_data = glue_builder.as_dataset(split='validation')\n",
        "\n",
        "# è½¬æ¢ä¸ºç‰¹å¾çš„å‡½æ•°\n",
        "def convert_dataset_to_features(dataset, tokenizer, max_length, task, num_examples=None):\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    # ä»æ•°æ®é›†ä¸­æå–æ–‡æœ¬å’Œæ ‡ç­¾\n",
        "    for example in dataset.take(num_examples or float('inf')):\n",
        "        text = example['sentence'].numpy().decode('utf-8')\n",
        "        label = example['label'].numpy()\n",
        "        texts.append(text)\n",
        "        labels.append(label)\n",
        "\n",
        "    # ä½¿ç”¨tokenizerå¤„ç†æ–‡æœ¬\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # åˆ›å»ºç‰¹å¾æ•°æ®é›†\n",
        "    dataset = tf.data.Dataset.from_tensor_slices({\n",
        "        'input_ids': encoded['input_ids'],\n",
        "        'attention_mask': encoded['attention_mask'],\n",
        "        'labels': tf.convert_to_tensor(labels, dtype=tf.int32)\n",
        "    })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# è®¡ç®—è¦ä½¿ç”¨çš„æ•°æ®é‡\n",
        "num_train_to_use = int(num_train_examples * (use_percentage_of_data/100))\n",
        "num_dev_to_use = int(num_dev_examples * (use_percentage_of_data/100))\n",
        "\n",
        "# è½¬æ¢æ•°æ®é›†ä¸ºç‰¹å¾\n",
        "train_dataset = convert_dataset_to_features(\n",
        "    train_data, tokenizer, max_length=max_seq_length, task='sst-2', num_examples=num_train_to_use\n",
        ")\n",
        "train_dataset = train_dataset.shuffle(100).batch(train_batch_size)\n",
        "\n",
        "dev_dataset = convert_dataset_to_features(\n",
        "    dev_data, tokenizer, max_length=max_seq_length, task='sst-2', num_examples=num_dev_to_use\n",
        ")\n",
        "dev_dataset = dev_dataset.batch(eval_batch_size)\n",
        "\n",
        "# Map the labels for printing\n",
        "label_mapping = {i: glue_builder.info.features['label'].int2str(i) for i in range(num_labels)}\n",
        "\n",
        "print(f'\\n\\nThe dataset is downloaded. The entire dataset has {num_train_examples + num_dev_examples} examples of which you are using {use_percentage_of_data}%. This will result in a train dataset with {num_train_to_use} examples and a validation dataset with {num_dev_to_use} examples.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The dataset is downloaded. The entire dataset has 68221 examples of which you are using 10%. This will result in a train dataset with 6734 examples and a validation dataset with 87 examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2AsubSgKGDu"
      },
      "source": [
        "# Compile the Model, Train it on the SST-2 Task and Save the Result\n",
        "You can skip this step if you are using the already finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XSbsZFDQTEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547b12c7-8faf-4723-af85-955ae3ab5f72"
      },
      "source": [
        "#@markdown >The default learning rate of 2e5 will be fine in most cases\n",
        "learning_rate = 2e-5 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown > Typically these type of models are finetuned for 3 epochs. This can be increased for small datasets and decreased for large datasets.\n",
        "num_epochs = 1  #@param {type: \"integer\"}\n",
        "\n",
        "# Initialise a Model for Sequence Classification with 2 labels\n",
        "config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Metrics and callbacks\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "checkpoint_path = './checkpoints/checkpoint.{epoch:02d}'\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True)]\n",
        "\n",
        "# Compute some variables\n",
        "train_steps_per_epoch = int(num_train_examples * (use_percentage_of_data/100) / train_batch_size)\n",
        "dev_steps_per_epoch = int(num_dev_examples * (use_percentage_of_data/100) / eval_batch_size)\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset,\n",
        "  epochs=num_epochs,\n",
        "  steps_per_epoch=train_steps_per_epoch,\n",
        "  validation_data=dev_dataset,\n",
        "  validation_steps=dev_steps_per_epoch,\n",
        "  callbacks=callbacks)\n",
        "\n",
        "# Print some information about the training\n",
        "print(f'\\nThe training has finished training after {num_epochs} epochs.')\n",
        "print('\\nThe history contains the accuracy and loss at every epoch:')\n",
        "print(json.dumps(history.history, indent=4))\n",
        "\n",
        "print('\\nThe checkpoint callback has generated a checkpoint after every epoch (loss being the training loss, val_loss is the validation loss):')\n",
        "!ls -lha ./checkpoints/\n",
        "\n",
        "print('\\nWe will now save the finetuned model and the corresponding config file on your Colab disk.')\n",
        "model.save_pretrained('./huggingface_model/')\n",
        "\n",
        "print('\\nTensorflow model and config-file is saved in ./huggingface_model/')\n",
        "!ls -lha ./huggingface_model/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at digitalepidemiologylab/covid-twitter-bert and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "841/841 [==============================] - 737s 765ms/step - loss: 0.3116 - accuracy: 0.8691 - val_loss: 0.2918 - val_accuracy: 0.9250\n",
            "\n",
            "The training has finished training after 1 epochs.\n",
            "\n",
            "The history contains the accuracy and loss at every epoch:\n",
            "{\n",
            "    \"loss\": [\n",
            "        0.31164631247520447\n",
            "    ],\n",
            "    \"accuracy\": [\n",
            "        0.8690546751022339\n",
            "    ],\n",
            "    \"val_loss\": [\n",
            "        0.2917918562889099\n",
            "    ],\n",
            "    \"val_accuracy\": [\n",
            "        0.925000011920929\n",
            "    ]\n",
            "}\n",
            "\n",
            "The checkpoint callback has generated a checkpoint after every epoch (loss being the training loss, val_loss is the validation loss):\n",
            "total 3.8G\n",
            "drwxr-xr-x 2 root root 4.0K May 13 06:13 .\n",
            "drwxr-xr-x 1 root root 4.0K May 13 06:00 ..\n",
            "-rw-r--r-- 1 root root   83 May 13 06:13 checkpoint\n",
            "-rw-r--r-- 1 root root 3.8G May 13 06:13 checkpoint.01.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root  73K May 13 06:13 checkpoint.01.index\n",
            "\n",
            "We will now save the finetuned model and the corresponding config file on your Colab disk.\n",
            "\n",
            "Tensorflow model and config-file is saved in ./huggingface_model/\n",
            "total 1.3G\n",
            "drwxr-xr-x 2 root root 4.0K May 13 06:13 .\n",
            "drwxr-xr-x 1 root root 4.0K May 13 06:13 ..\n",
            "-rw-r--r-- 1 root root  611 May 13 06:13 config.json\n",
            "-rw-r--r-- 1 root root 1.3G May 13 06:14 tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYamwzWeM9vM"
      },
      "source": [
        "# Predict\n",
        "Let's run some inference with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T0zy0fK9Rmx"
      },
      "source": [
        "# Small function only used for formatting the output\n",
        "def format_prediction(preds, label_mapping, label_name):\n",
        "    preds = tf.nn.softmax(preds, axis=1)\n",
        "    formatted_preds = []\n",
        "    for pred in preds.numpy():\n",
        "        # convert to Python types and sort\n",
        "        pred = {label: float(probability) for label, probability in zip(label_mapping.values(), pred)}\n",
        "        pred = {k: v for k, v in sorted(pred.items(), key=lambda item: item[1], reverse=True)}\n",
        "        formatted_preds.append({label_name: list(pred.keys())[0], f'{label_name}_probabilities': pred})\n",
        "    return formatted_preds"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdk8zasnKJWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168e3552-2a57-46e9-f9e2-8b7474afa857"
      },
      "source": [
        "#@markdown >Please input text that the model can try to classify\n",
        "input_text = 'I want to get vaccines.'  #@param {type: \"string\"}\n",
        "\n",
        "# Tokenize the input\n",
        "input_ids = tf.constant(tokenizer.encode(input_text, add_special_tokens=True))[None, :]\n",
        "\n",
        "# Run predictions\n",
        "preds = model(input_ids)\n",
        "\n",
        "# format logits\n",
        "formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "print(f'\\nLabel Mapping:{json.dumps(label_mapping, indent=4)}')\n",
        "print(f'\\nLogits: {preds}')\n",
        "print(f'\\nProbabilities:{json.dumps(formatted_preds, indent=4)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Label Mapping:{\n",
            "    \"0\": \"negative\",\n",
            "    \"1\": \"positive\"\n",
            "}\n",
            "\n",
            "Logits: TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.49143448,  0.5510604 ]], dtype=float32)>, hidden_states=None, attentions=None)\n",
            "\n",
            "Probabilities:[\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.739331066608429,\n",
            "            \"negative\": 0.26066890358924866\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBq1C2U9ScWg"
      },
      "source": [
        "##### Copyright 2020 Per Egil Kummervold and Martin MÃ¼ller"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# å‡è®¾è¿™äº›å‡½æ•°å’Œå˜é‡å·²ç»å®šä¹‰\n",
        "# tokenizer = ...\n",
        "# model = ...\n",
        "# label_mapping = ...\n",
        "# format_prediction = ...\n",
        "\n",
        "def analyze_texts(texts):\n",
        "    \"\"\"å¯¹å¤šæ¡æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†æå¹¶æŒ‰ç§¯æç¨‹åº¦é™åºæ’å\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯\n",
        "        input_ids = tf.constant(tokenizer.encode(text, add_special_tokens=True))[None, :]\n",
        "\n",
        "        # æ¨¡å‹é¢„æµ‹\n",
        "        preds = model(input_ids)\n",
        "\n",
        "        # æ ¼å¼åŒ–é¢„æµ‹ç»“æœ\n",
        "        formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "        # æå–ç§¯ææƒ…æ„Ÿçš„æ¦‚ç‡å€¼\n",
        "        try:\n",
        "            # ä»åµŒå¥—ç»“æ„ä¸­è·å–ç§¯ææƒ…æ„Ÿæ¦‚ç‡\n",
        "            positive_probability = formatted_preds[0]['sentiment_probabilities']['positive']\n",
        "        except (IndexError, KeyError, TypeError):\n",
        "            print(f\"è­¦å‘Š: æ— æ³•è§£ææ–‡æœ¬ '{text[:30]}...' çš„é¢„æµ‹ç»“æœ\")\n",
        "            positive_probability = 0.0\n",
        "\n",
        "        # å­˜å‚¨ç»“æœ\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'positive_probability': positive_probability,\n",
        "            'all_predictions': formatted_preds\n",
        "        })\n",
        "\n",
        "    # æŒ‰ç§¯ææ¦‚ç‡é™åºæ’åº\n",
        "    sorted_results = sorted(results, key=lambda x: x['positive_probability'], reverse=True)\n",
        "\n",
        "    return sorted_results\n",
        "\n",
        "# ç¤ºä¾‹ä½¿ç”¨\n",
        "sample_texts = [\n",
        "    \"Vaccines are a great achievement for public health!\",\n",
        "    \"I'm not sure about getting vaccinated.\",\n",
        "    \"Getting vaccinated is the best way to protect ourselves.\",\n",
        "    \"Vaccines might have some side effects, but they're generally safe.\"\n",
        "]\n",
        "\n",
        "# åˆ†æå¹¶æ’åºæ–‡æœ¬\n",
        "ranked_texts = analyze_texts(sample_texts)\n",
        "\n",
        "# æ‰“å°æ’åç»“æœ\n",
        "print(\"\\nTexts ranked by positive sentiment (descending):\")\n",
        "for i, result in enumerate(ranked_texts, 1):\n",
        "    print(f\"\\nRank {i}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Positive Probability: {result['positive_probability']:.4f}\")\n",
        "    print(f\"All Predictions: {json.dumps(result['all_predictions'], indent=4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge4mZcI-9frp",
        "outputId": "62c3286b-1926-41aa-8191-9fb81eb0e621"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texts ranked by positive sentiment (descending):\n",
            "\n",
            "Rank 1:\n",
            "Text: Vaccines are a great achievement for public health!\n",
            "Positive Probability: 0.9929\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.99289470911026,\n",
            "            \"negative\": 0.007105330936610699\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 2:\n",
            "Text: Vaccines might have some side effects, but they're generally safe.\n",
            "Positive Probability: 0.9554\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9553871750831604,\n",
            "            \"negative\": 0.04461280629038811\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 3:\n",
            "Text: Getting vaccinated is the best way to protect ourselves.\n",
            "Positive Probability: 0.9049\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9049472808837891,\n",
            "            \"negative\": 0.09505265951156616\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 4:\n",
            "Text: I'm not sure about getting vaccinated.\n",
            "Positive Probability: 0.2378\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.7621815204620361,\n",
            "            \"positive\": 0.23781844973564148\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# å‡è®¾è¿™äº›å‡½æ•°å’Œå˜é‡å·²ç»å®šä¹‰\n",
        "# tokenizer = ...\n",
        "# model = ...\n",
        "# label_mapping = ...\n",
        "# format_prediction = ...\n",
        "\n",
        "def analyze_texts(texts):\n",
        "    \"\"\"å¯¹å¤šæ¡æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†æå¹¶æŒ‰ç§¯æç¨‹åº¦é™åºæ’å\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯\n",
        "        input_ids = tf.constant(tokenizer.encode(text, add_special_tokens=True))[None, :]\n",
        "\n",
        "        # æ¨¡å‹é¢„æµ‹\n",
        "        preds = model(input_ids)\n",
        "\n",
        "        # æ ¼å¼åŒ–é¢„æµ‹ç»“æœ\n",
        "        formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "        # æå–ç§¯ææƒ…æ„Ÿçš„æ¦‚ç‡å€¼\n",
        "        try:\n",
        "            # ä»åµŒå¥—ç»“æ„ä¸­è·å–ç§¯ææƒ…æ„Ÿæ¦‚ç‡\n",
        "            positive_probability = formatted_preds[0]['sentiment_probabilities']['positive']\n",
        "        except (IndexError, KeyError, TypeError):\n",
        "            print(f\"è­¦å‘Š: æ— æ³•è§£ææ–‡æœ¬ '{text[:30]}...' çš„é¢„æµ‹ç»“æœ\")\n",
        "            positive_probability = 0.0\n",
        "\n",
        "        # å­˜å‚¨ç»“æœ\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'positive_probability': positive_probability,\n",
        "            'all_predictions': formatted_preds\n",
        "        })\n",
        "\n",
        "    # æŒ‰ç§¯ææ¦‚ç‡é™åºæ’åº\n",
        "    sorted_results = sorted(results, key=lambda x: x['positive_probability'], reverse=True)\n",
        "\n",
        "    return sorted_results\n",
        "\n",
        "# ç¤ºä¾‹ä½¿ç”¨\n",
        "sample_texts = [\n",
        "   \"This pandemic situation is really getting better!\",\n",
        "    \"This pandemic situation is really getting worse!\",\n",
        "    \"I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated or not.\",\n",
        "]\n",
        "\n",
        "# åˆ†æå¹¶æ’åºæ–‡æœ¬\n",
        "ranked_texts = analyze_texts(sample_texts)\n",
        "\n",
        "# æ‰“å°æ’åç»“æœ\n",
        "print(\"\\nTexts ranked by positive sentiment (descending):\")\n",
        "for i, result in enumerate(ranked_texts, 1):\n",
        "    print(f\"\\nRank {i}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Positive Probability: {result['positive_probability']:.4f}\")\n",
        "    print(f\"All Predictions: {json.dumps(result['all_predictions'], indent=4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx6qYks1-_mz",
        "outputId": "0a3a6381-4169-473f-bd90-f5fe1ae9c676"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texts ranked by positive sentiment (descending):\n",
            "\n",
            "Rank 1:\n",
            "Text: This pandemic situation is really getting better!\n",
            "Positive Probability: 0.9842\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9841827750205994,\n",
            "            \"negative\": 0.01581723988056183\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 2:\n",
            "Text: I want to get vaccinated.\n",
            "Positive Probability: 0.7110\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.7109977006912231,\n",
            "            \"negative\": 0.28900235891342163\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 3:\n",
            "Text: I am not sure if I want to get vaccinated or not.\n",
            "Positive Probability: 0.2597\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.7403048276901245,\n",
            "            \"positive\": 0.2596952021121979\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 4:\n",
            "Text: I am not sure if I want to get vaccinated.\n",
            "Positive Probability: 0.1747\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8252729177474976,\n",
            "            \"positive\": 0.17472711205482483\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 5:\n",
            "Text: This pandemic situation is really getting worse!\n",
            "Positive Probability: 0.0369\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.9631170630455017,\n",
            "            \"positive\": 0.03688299283385277\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# å‡è®¾è¿™äº›å‡½æ•°å’Œå˜é‡å·²ç»å®šä¹‰\n",
        "# tokenizer = ...\n",
        "# model = ...\n",
        "# label_mapping = ...\n",
        "# format_prediction = ...\n",
        "\n",
        "def analyze_texts(texts):\n",
        "    \"\"\"å¯¹å¤šæ¡æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†æå¹¶æŒ‰ç§¯æç¨‹åº¦é™åºæ’å\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯\n",
        "        input_ids = tf.constant(tokenizer.encode(text, add_special_tokens=True))[None, :]\n",
        "\n",
        "        # æ¨¡å‹é¢„æµ‹\n",
        "        preds = model(input_ids)\n",
        "\n",
        "        # æ ¼å¼åŒ–é¢„æµ‹ç»“æœ\n",
        "        formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "        # æå–ç§¯ææƒ…æ„Ÿçš„æ¦‚ç‡å€¼\n",
        "        try:\n",
        "            # ä»åµŒå¥—ç»“æ„ä¸­è·å–ç§¯ææƒ…æ„Ÿæ¦‚ç‡\n",
        "            positive_probability = formatted_preds[0]['sentiment_probabilities']['positive']\n",
        "        except (IndexError, KeyError, TypeError):\n",
        "            print(f\"è­¦å‘Š: æ— æ³•è§£ææ–‡æœ¬ '{text[:30]}...' çš„é¢„æµ‹ç»“æœ\")\n",
        "            positive_probability = 0.0\n",
        "\n",
        "        # å­˜å‚¨ç»“æœ\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'positive_probability': positive_probability,\n",
        "            'all_predictions': formatted_preds\n",
        "        })\n",
        "\n",
        "    # æŒ‰ç§¯ææ¦‚ç‡é™åºæ’åº\n",
        "    sorted_results = sorted(results, key=lambda x: x['positive_probability'], reverse=True)\n",
        "\n",
        "    return sorted_results\n",
        "\n",
        "# ç¤ºä¾‹ä½¿ç”¨\n",
        "sample_texts = [\n",
        "   \"This pandemic situation is really getting better!\",\n",
        "    \"This pandemic situation is really getting worse!\",\n",
        "    \"I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated or not.\",\n",
        "   \"This pandemic situation is really getting better!ğŸ˜\"\n",
        "]\n",
        "\n",
        "# åˆ†æå¹¶æ’åºæ–‡æœ¬\n",
        "ranked_texts = analyze_texts(sample_texts)\n",
        "\n",
        "# æ‰“å°æ’åç»“æœ\n",
        "print(\"\\nTexts ranked by positive sentiment (descending):\")\n",
        "for i, result in enumerate(ranked_texts, 1):\n",
        "    print(f\"\\nRank {i}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Positive Probability: {result['positive_probability']:.4f}\")\n",
        "    print(f\"All Predictions: {json.dumps(result['all_predictions'], indent=4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2ciGb6a_XGQ",
        "outputId": "1a87f7dd-9b63-4e46-cea5-31f0d653e926"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texts ranked by positive sentiment (descending):\n",
            "\n",
            "Rank 1:\n",
            "Text: This pandemic situation is really getting better!\n",
            "Positive Probability: 0.9807\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9806835055351257,\n",
            "            \"negative\": 0.019316459074616432\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 2:\n",
            "Text: This pandemic situation is really getting better!ğŸ˜\n",
            "Positive Probability: 0.9591\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9590954780578613,\n",
            "            \"negative\": 0.04090452939271927\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 3:\n",
            "Text: I want to get vaccinated.\n",
            "Positive Probability: 0.4858\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.5141684412956238,\n",
            "            \"positive\": 0.4858315885066986\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 4:\n",
            "Text: I am not sure if I want to get vaccinated.\n",
            "Positive Probability: 0.1100\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8899889588356018,\n",
            "            \"positive\": 0.11001105606555939\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 5:\n",
            "Text: I am not sure if I want to get vaccinated or not.\n",
            "Positive Probability: 0.1070\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8929714560508728,\n",
            "            \"positive\": 0.10702849924564362\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 6:\n",
            "Text: This pandemic situation is really getting worse!\n",
            "Positive Probability: 0.0592\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.9407861232757568,\n",
            "            \"positive\": 0.059213921427726746\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su9LMT22_mYi",
        "outputId": "e4b75783-6c92-4839-93be-7cd75888c424"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# è·å–ç‰ˆæœ¬\n",
        "transformers_version = transformers.__version__\n",
        "tensorflow_version = tf.__version__\n",
        "torch_version = torch.__version__\n",
        "\n",
        "# ä¿å­˜è·¯å¾„\n",
        "save_path = \"/content/drive/MyDrive/saved_model/\"  # æ›¿æ¢ä¸ºä½ çš„ä¿å­˜è·¯å¾„\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# å†™å…¥ requirements.txt\n",
        "with open(os.path.join(save_path, \"requirements.txt\"), \"w\") as f:\n",
        "    f.write(f\"transformers=={transformers_version}\\n\")\n",
        "    f.write(f\"tensorflow=={tensorflow_version}\\n\")\n",
        "    f.write(f\"torch=={torch_version}\\n\")\n",
        "    # æ·»åŠ å…¶ä»–ä¾èµ–ï¼ˆå¦‚æœæœ‰ï¼‰\n",
        "\n",
        "print(f\"ç‰ˆæœ¬ä¿¡æ¯å·²ä¿å­˜åˆ°: {save_path}/requirements.txt\")"
      ],
      "metadata": {
        "id": "NgQk2uHzTJFO",
        "outputId": "bb416407-4397-4f4f-891d-ee34cf0268f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç‰ˆæœ¬ä¿¡æ¯å·²ä¿å­˜åˆ°: /content/drive/MyDrive/saved_model//requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "\n",
        "# ä¿å­˜è·¯å¾„\n",
        "save_path = \"/content/drive/MyDrive/saved_model/\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# ä¿å­˜æ¨¡å‹æƒé‡å’Œé…ç½®\n",
        "model.save_pretrained(save_path)\n",
        "\n",
        "# ä¿å­˜åˆ†è¯å™¨ï¼ˆä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„ç›¸åŒï¼‰\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert\")  # æ›¿æ¢ä¸ºä½ è®­ç»ƒæ—¶ä½¿ç”¨çš„åˆ†è¯å™¨\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# ä¿å­˜è®­ç»ƒå‚æ•°ï¼ˆåŒ…å«æ‰€æœ‰å…³é”®é…ç½®ï¼‰\n",
        "training_args = {\n",
        "    \"model_name\": model_name,  # ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹åç§°\n",
        "    \"num_labels\": num_labels,  # åˆ†ç±»ä»»åŠ¡çš„æ ‡ç­¾æ•°é‡\n",
        "    \"max_seq_length\": max_seq_length,  # æœ€å¤§åºåˆ—é•¿åº¦\n",
        "    \"train_batch_size\": train_batch_size,  # è®­ç»ƒæ‰¹é‡å¤§å°\n",
        "    \"eval_batch_size\": eval_batch_size,  # è¯„ä¼°æ‰¹é‡å¤§å°\n",
        "    \"use_percentage_of_data\": use_percentage_of_data,  # ä½¿ç”¨æ•°æ®çš„ç™¾åˆ†æ¯”\n",
        "    \"learning_rate\": learning_rate,  # å­¦ä¹ ç‡\n",
        "    \"num_epochs\": num_epochs,  # è®­ç»ƒè½®æ•°\n",
        "    \"num_train_examples\": num_train_examples,  # è®­ç»ƒæ ·æœ¬æ€»æ•°\n",
        "    \"num_dev_examples\": num_dev_examples,  # éªŒè¯æ ·æœ¬æ€»æ•°\n",
        "    \"train_steps_per_epoch\": train_steps_per_epoch,  # æ¯è½®è®­ç»ƒæ­¥æ•°\n",
        "    \"dev_steps_per_epoch\": dev_steps_per_epoch,  # æ¯è½®éªŒè¯æ­¥æ•°\n",
        "    \"optimizer\": optimizer.get_config(),  # ä¼˜åŒ–å™¨é…ç½®\n",
        "    \"loss\": loss.__class__.__name__,  # æŸå¤±å‡½æ•°åç§°\n",
        "    \"metrics\": [m.name for m in metrics],  # è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨\n",
        "}\n",
        "\n",
        "import pickle\n",
        "\n",
        "# ä¿å­˜è®­ç»ƒå‚æ•°åˆ° pickle æ–‡ä»¶\n",
        "with open(os.path.join(save_path, \"training_args.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(training_args, f)\n",
        "\n",
        "print(f\"è®­ç»ƒå‚æ•°å·²ä¿å­˜åˆ°: {save_path}/training_args.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "iiKtZI5KJ57d",
        "outputId": "ee1acd9d-1e16-4501-e2ab-0589d7344a00"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type float32 is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2001d02db243>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_args.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"è®­ç»ƒå‚æ•°å·²ä¿å­˜åˆ°: {save_path}/training_args.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type float32 is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZh7XAsFVR6h",
        "outputId": "e8317058-6ad1-471a-d4ea-f3b542331abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "è®­ç»ƒå‚æ•°å·²ä¿å­˜åˆ°: /content/drive/MyDrive/saved_model//training_args.pkl\n"
          ]
        }
      ]
    }
  ]
}