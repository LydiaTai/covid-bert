{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CT-BERT - Huggingface (GPU training)",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LydiaTai/covid-bert/blob/main/CT_BERT_Huggingface_(GPU_training).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3gcwjDTd4oY"
      },
      "source": [
        "<img align=\"right\" width=\"450px\" src=\"https://github.com/digitalepidemiologylab/covid-twitter-bert/raw/master/images/COVID-Twitter-BERT-medium.png\">\n",
        "\n",
        "# Finetuning COVID-Twitter-BERT using Huggingface\n",
        "In this notebook we will finetune CT-BERT for sentiment classification using the transformer library by Huggingface.\n",
        "\n",
        "Learn more about this library [here](https://huggingface.co/transformers/).\n",
        "\n",
        "## Before proceeding\n",
        "Create a copy of this notebook by going to \"File - Save a Copy in Drive\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVsmxmSberX2"
      },
      "source": [
        "# Install transformers and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_k-GwPM9JXP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "80a7a970-336e-409b-e7db-da5df62c8b92"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 5.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 15.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=d717d2618d9a205fbddae8ca2ee7be9e7110728eadfbd8b187b3767fe071ad88\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvvPnOFQH2pR"
      },
      "source": [
        "from transformers import (\n",
        "   AutoConfig,\n",
        "   AutoTokenizer,\n",
        "   TFAutoModelForSequenceClassification,\n",
        "   glue_convert_examples_to_features\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrDpLTI7Jt5p"
      },
      "source": [
        "# Choose a Model from the Huggingface Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyLIi4bvImJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74037f0-6adc-4869-d776-bd86c3c03109"
      },
      "source": [
        "# Choose model\n",
        "# @markdown >The default model is <i><b>COVID-Twitter-BERT</b></i>. You can however choose <i><b>BERT Base</i></b> or <i><b>BERT Large</i></b> to compare these models to the <i><b>COVID-Twitter-BERT</i></b>. All these three models will be initiated with a random classification layer. If you go directly to the Predict-cell after having compiled the model, you will see that it still runs the predition. However the output will be random. The training steps below will finetune this for the specific task. <br /><br />\n",
        "model_name = 'digitalepidemiologylab/covid-twitter-bert' #@param [\"digitalepidemiologylab/covid-twitter-bert\", \"bert-large-uncased\", \"bert-base-uncased\"]\n",
        "\n",
        "# Initialise tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdiZDL69JpNC"
      },
      "source": [
        "# Download the SST-2 Dataset and Prepare for Finetuning\n",
        "You can skip this step if you are using the already finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11kI0lhIJoxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780e3cc6-0601-4936-8726-e82f57b04fde"
      },
      "source": [
        "# Parameters\n",
        "#@markdown >Batch size and sequence length needs to be set to prepare the data. The size of the batches depends on available memory. For Colab GPU limit batch size to 8 and sequence length to 96. By reducing the length of the input (max_seq_length) you can also increase the batch size. For a dataset like SST-2 with lots of short sentences. this will likely benefit training.\n",
        "max_seq_length = 96 #@param {type: \"integer\"}\n",
        "train_batch_size =  8#@param {type: \"integer\"}\n",
        "eval_batch_size = 8 #@param {type: \"integer\"}\n",
        "\n",
        "#@markdown >The Glue dataset has around 62000 examples, and we really do not need them all for training a decent model. To cut down training time, please reduse this to only a percentage of the entire set.\n",
        "use_percentage_of_data = 10\n",
        "\n",
        "# get dataset sizes\n",
        "glue_builder = tfds.builder('glue/sst2')  # 注意这里改为sst2\n",
        "glue_builder.download_and_prepare() # This line was moved up\n",
        "num_train_examples = glue_builder.info.splits['train'].num_examples\n",
        "num_dev_examples = glue_builder.info.splits['validation'].num_examples\n",
        "num_labels = glue_builder.info.features['label'].num_classes\n",
        "\n",
        "# download datasets\n",
        "glue_builder.download_and_prepare()\n",
        "train_data = glue_builder.as_dataset(split='train')\n",
        "dev_data = glue_builder.as_dataset(split='validation')\n",
        "\n",
        "# 转换为特征的函数\n",
        "def convert_dataset_to_features(dataset, tokenizer, max_length, task, num_examples=None):\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    # 从数据集中提取文本和标签\n",
        "    for example in dataset.take(num_examples or float('inf')):\n",
        "        text = example['sentence'].numpy().decode('utf-8')\n",
        "        label = example['label'].numpy()\n",
        "        texts.append(text)\n",
        "        labels.append(label)\n",
        "\n",
        "    # 使用tokenizer处理文本\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # 创建特征数据集\n",
        "    dataset = tf.data.Dataset.from_tensor_slices({\n",
        "        'input_ids': encoded['input_ids'],\n",
        "        'attention_mask': encoded['attention_mask'],\n",
        "        'labels': tf.convert_to_tensor(labels, dtype=tf.int32)\n",
        "    })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# 计算要使用的数据量\n",
        "num_train_to_use = int(num_train_examples * (use_percentage_of_data/100))\n",
        "num_dev_to_use = int(num_dev_examples * (use_percentage_of_data/100))\n",
        "\n",
        "# 转换数据集为特征\n",
        "train_dataset = convert_dataset_to_features(\n",
        "    train_data, tokenizer, max_length=max_seq_length, task='sst-2', num_examples=num_train_to_use\n",
        ")\n",
        "train_dataset = train_dataset.shuffle(100).batch(train_batch_size)\n",
        "\n",
        "dev_dataset = convert_dataset_to_features(\n",
        "    dev_data, tokenizer, max_length=max_seq_length, task='sst-2', num_examples=num_dev_to_use\n",
        ")\n",
        "dev_dataset = dev_dataset.batch(eval_batch_size)\n",
        "\n",
        "# Map the labels for printing\n",
        "label_mapping = {i: glue_builder.info.features['label'].int2str(i) for i in range(num_labels)}\n",
        "\n",
        "print(f'\\n\\nThe dataset is downloaded. The entire dataset has {num_train_examples + num_dev_examples} examples of which you are using {use_percentage_of_data}%. This will result in a train dataset with {num_train_to_use} examples and a validation dataset with {num_dev_to_use} examples.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The dataset is downloaded. The entire dataset has 68221 examples of which you are using 10%. This will result in a train dataset with 6734 examples and a validation dataset with 87 examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2AsubSgKGDu"
      },
      "source": [
        "# Compile the Model, Train it on the SST-2 Task and Save the Result\n",
        "You can skip this step if you are using the already finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XSbsZFDQTEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547b12c7-8faf-4723-af85-955ae3ab5f72"
      },
      "source": [
        "#@markdown >The default learning rate of 2e5 will be fine in most cases\n",
        "learning_rate = 2e-5 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown > Typically these type of models are finetuned for 3 epochs. This can be increased for small datasets and decreased for large datasets.\n",
        "num_epochs = 1  #@param {type: \"integer\"}\n",
        "\n",
        "# Initialise a Model for Sequence Classification with 2 labels\n",
        "config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Metrics and callbacks\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "checkpoint_path = './checkpoints/checkpoint.{epoch:02d}'\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True)]\n",
        "\n",
        "# Compute some variables\n",
        "train_steps_per_epoch = int(num_train_examples * (use_percentage_of_data/100) / train_batch_size)\n",
        "dev_steps_per_epoch = int(num_dev_examples * (use_percentage_of_data/100) / eval_batch_size)\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset,\n",
        "  epochs=num_epochs,\n",
        "  steps_per_epoch=train_steps_per_epoch,\n",
        "  validation_data=dev_dataset,\n",
        "  validation_steps=dev_steps_per_epoch,\n",
        "  callbacks=callbacks)\n",
        "\n",
        "# Print some information about the training\n",
        "print(f'\\nThe training has finished training after {num_epochs} epochs.')\n",
        "print('\\nThe history contains the accuracy and loss at every epoch:')\n",
        "print(json.dumps(history.history, indent=4))\n",
        "\n",
        "print('\\nThe checkpoint callback has generated a checkpoint after every epoch (loss being the training loss, val_loss is the validation loss):')\n",
        "!ls -lha ./checkpoints/\n",
        "\n",
        "print('\\nWe will now save the finetuned model and the corresponding config file on your Colab disk.')\n",
        "model.save_pretrained('./huggingface_model/')\n",
        "\n",
        "print('\\nTensorflow model and config-file is saved in ./huggingface_model/')\n",
        "!ls -lha ./huggingface_model/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at digitalepidemiologylab/covid-twitter-bert and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "841/841 [==============================] - 737s 765ms/step - loss: 0.3116 - accuracy: 0.8691 - val_loss: 0.2918 - val_accuracy: 0.9250\n",
            "\n",
            "The training has finished training after 1 epochs.\n",
            "\n",
            "The history contains the accuracy and loss at every epoch:\n",
            "{\n",
            "    \"loss\": [\n",
            "        0.31164631247520447\n",
            "    ],\n",
            "    \"accuracy\": [\n",
            "        0.8690546751022339\n",
            "    ],\n",
            "    \"val_loss\": [\n",
            "        0.2917918562889099\n",
            "    ],\n",
            "    \"val_accuracy\": [\n",
            "        0.925000011920929\n",
            "    ]\n",
            "}\n",
            "\n",
            "The checkpoint callback has generated a checkpoint after every epoch (loss being the training loss, val_loss is the validation loss):\n",
            "total 3.8G\n",
            "drwxr-xr-x 2 root root 4.0K May 13 06:13 .\n",
            "drwxr-xr-x 1 root root 4.0K May 13 06:00 ..\n",
            "-rw-r--r-- 1 root root   83 May 13 06:13 checkpoint\n",
            "-rw-r--r-- 1 root root 3.8G May 13 06:13 checkpoint.01.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root  73K May 13 06:13 checkpoint.01.index\n",
            "\n",
            "We will now save the finetuned model and the corresponding config file on your Colab disk.\n",
            "\n",
            "Tensorflow model and config-file is saved in ./huggingface_model/\n",
            "total 1.3G\n",
            "drwxr-xr-x 2 root root 4.0K May 13 06:13 .\n",
            "drwxr-xr-x 1 root root 4.0K May 13 06:13 ..\n",
            "-rw-r--r-- 1 root root  611 May 13 06:13 config.json\n",
            "-rw-r--r-- 1 root root 1.3G May 13 06:14 tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYamwzWeM9vM"
      },
      "source": [
        "# Predict\n",
        "Let's run some inference with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T0zy0fK9Rmx"
      },
      "source": [
        "# Small function only used for formatting the output\n",
        "def format_prediction(preds, label_mapping, label_name):\n",
        "    preds = tf.nn.softmax(preds, axis=1)\n",
        "    formatted_preds = []\n",
        "    for pred in preds.numpy():\n",
        "        # convert to Python types and sort\n",
        "        pred = {label: float(probability) for label, probability in zip(label_mapping.values(), pred)}\n",
        "        pred = {k: v for k, v in sorted(pred.items(), key=lambda item: item[1], reverse=True)}\n",
        "        formatted_preds.append({label_name: list(pred.keys())[0], f'{label_name}_probabilities': pred})\n",
        "    return formatted_preds"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdk8zasnKJWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168e3552-2a57-46e9-f9e2-8b7474afa857"
      },
      "source": [
        "#@markdown >Please input text that the model can try to classify\n",
        "input_text = 'I want to get vaccines.'  #@param {type: \"string\"}\n",
        "\n",
        "# Tokenize the input\n",
        "input_ids = tf.constant(tokenizer.encode(input_text, add_special_tokens=True))[None, :]\n",
        "\n",
        "# Run predictions\n",
        "preds = model(input_ids)\n",
        "\n",
        "# format logits\n",
        "formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "print(f'\\nLabel Mapping:{json.dumps(label_mapping, indent=4)}')\n",
        "print(f'\\nLogits: {preds}')\n",
        "print(f'\\nProbabilities:{json.dumps(formatted_preds, indent=4)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Label Mapping:{\n",
            "    \"0\": \"negative\",\n",
            "    \"1\": \"positive\"\n",
            "}\n",
            "\n",
            "Logits: TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.49143448,  0.5510604 ]], dtype=float32)>, hidden_states=None, attentions=None)\n",
            "\n",
            "Probabilities:[\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.739331066608429,\n",
            "            \"negative\": 0.26066890358924866\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBq1C2U9ScWg"
      },
      "source": [
        "##### Copyright 2020 Per Egil Kummervold and Martin Müller"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# 假设这些函数和变量已经定义\n",
        "# tokenizer = ...\n",
        "# model = ...\n",
        "# label_mapping = ...\n",
        "# format_prediction = ...\n",
        "\n",
        "def analyze_texts(texts):\n",
        "    \"\"\"对多条文本进行情感分析并按积极程度降序排名\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        # 对输入文本进行分词\n",
        "        input_ids = tf.constant(tokenizer.encode(text, add_special_tokens=True))[None, :]\n",
        "\n",
        "        # 模型预测\n",
        "        preds = model(input_ids)\n",
        "\n",
        "        # 格式化预测结果\n",
        "        formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "        # 提取积极情感的概率值\n",
        "        try:\n",
        "            # 从嵌套结构中获取积极情感概率\n",
        "            positive_probability = formatted_preds[0]['sentiment_probabilities']['positive']\n",
        "        except (IndexError, KeyError, TypeError):\n",
        "            print(f\"警告: 无法解析文本 '{text[:30]}...' 的预测结果\")\n",
        "            positive_probability = 0.0\n",
        "\n",
        "        # 存储结果\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'positive_probability': positive_probability,\n",
        "            'all_predictions': formatted_preds\n",
        "        })\n",
        "\n",
        "    # 按积极概率降序排序\n",
        "    sorted_results = sorted(results, key=lambda x: x['positive_probability'], reverse=True)\n",
        "\n",
        "    return sorted_results\n",
        "\n",
        "# 示例使用\n",
        "sample_texts = [\n",
        "    \"Vaccines are a great achievement for public health!\",\n",
        "    \"I'm not sure about getting vaccinated.\",\n",
        "    \"Getting vaccinated is the best way to protect ourselves.\",\n",
        "    \"Vaccines might have some side effects, but they're generally safe.\"\n",
        "]\n",
        "\n",
        "# 分析并排序文本\n",
        "ranked_texts = analyze_texts(sample_texts)\n",
        "\n",
        "# 打印排名结果\n",
        "print(\"\\nTexts ranked by positive sentiment (descending):\")\n",
        "for i, result in enumerate(ranked_texts, 1):\n",
        "    print(f\"\\nRank {i}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Positive Probability: {result['positive_probability']:.4f}\")\n",
        "    print(f\"All Predictions: {json.dumps(result['all_predictions'], indent=4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge4mZcI-9frp",
        "outputId": "62c3286b-1926-41aa-8191-9fb81eb0e621"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texts ranked by positive sentiment (descending):\n",
            "\n",
            "Rank 1:\n",
            "Text: Vaccines are a great achievement for public health!\n",
            "Positive Probability: 0.9929\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.99289470911026,\n",
            "            \"negative\": 0.007105330936610699\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 2:\n",
            "Text: Vaccines might have some side effects, but they're generally safe.\n",
            "Positive Probability: 0.9554\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9553871750831604,\n",
            "            \"negative\": 0.04461280629038811\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 3:\n",
            "Text: Getting vaccinated is the best way to protect ourselves.\n",
            "Positive Probability: 0.9049\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9049472808837891,\n",
            "            \"negative\": 0.09505265951156616\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 4:\n",
            "Text: I'm not sure about getting vaccinated.\n",
            "Positive Probability: 0.2378\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.7621815204620361,\n",
            "            \"positive\": 0.23781844973564148\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# 假设这些函数和变量已经定义\n",
        "# tokenizer = ...\n",
        "# model = ...\n",
        "# label_mapping = ...\n",
        "# format_prediction = ...\n",
        "\n",
        "def analyze_texts(texts):\n",
        "    \"\"\"对多条文本进行情感分析并按积极程度降序排名\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        # 对输入文本进行分词\n",
        "        input_ids = tf.constant(tokenizer.encode(text, add_special_tokens=True))[None, :]\n",
        "\n",
        "        # 模型预测\n",
        "        preds = model(input_ids)\n",
        "\n",
        "        # 格式化预测结果\n",
        "        formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "        # 提取积极情感的概率值\n",
        "        try:\n",
        "            # 从嵌套结构中获取积极情感概率\n",
        "            positive_probability = formatted_preds[0]['sentiment_probabilities']['positive']\n",
        "        except (IndexError, KeyError, TypeError):\n",
        "            print(f\"警告: 无法解析文本 '{text[:30]}...' 的预测结果\")\n",
        "            positive_probability = 0.0\n",
        "\n",
        "        # 存储结果\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'positive_probability': positive_probability,\n",
        "            'all_predictions': formatted_preds\n",
        "        })\n",
        "\n",
        "    # 按积极概率降序排序\n",
        "    sorted_results = sorted(results, key=lambda x: x['positive_probability'], reverse=True)\n",
        "\n",
        "    return sorted_results\n",
        "\n",
        "# 示例使用\n",
        "sample_texts = [\n",
        "   \"This pandemic situation is really getting better!\",\n",
        "    \"This pandemic situation is really getting worse!\",\n",
        "    \"I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated or not.\",\n",
        "]\n",
        "\n",
        "# 分析并排序文本\n",
        "ranked_texts = analyze_texts(sample_texts)\n",
        "\n",
        "# 打印排名结果\n",
        "print(\"\\nTexts ranked by positive sentiment (descending):\")\n",
        "for i, result in enumerate(ranked_texts, 1):\n",
        "    print(f\"\\nRank {i}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Positive Probability: {result['positive_probability']:.4f}\")\n",
        "    print(f\"All Predictions: {json.dumps(result['all_predictions'], indent=4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx6qYks1-_mz",
        "outputId": "0a3a6381-4169-473f-bd90-f5fe1ae9c676"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texts ranked by positive sentiment (descending):\n",
            "\n",
            "Rank 1:\n",
            "Text: This pandemic situation is really getting better!\n",
            "Positive Probability: 0.9842\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9841827750205994,\n",
            "            \"negative\": 0.01581723988056183\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 2:\n",
            "Text: I want to get vaccinated.\n",
            "Positive Probability: 0.7110\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.7109977006912231,\n",
            "            \"negative\": 0.28900235891342163\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 3:\n",
            "Text: I am not sure if I want to get vaccinated or not.\n",
            "Positive Probability: 0.2597\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.7403048276901245,\n",
            "            \"positive\": 0.2596952021121979\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 4:\n",
            "Text: I am not sure if I want to get vaccinated.\n",
            "Positive Probability: 0.1747\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8252729177474976,\n",
            "            \"positive\": 0.17472711205482483\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 5:\n",
            "Text: This pandemic situation is really getting worse!\n",
            "Positive Probability: 0.0369\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.9631170630455017,\n",
            "            \"positive\": 0.03688299283385277\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# 假设这些函数和变量已经定义\n",
        "# tokenizer = ...\n",
        "# model = ...\n",
        "# label_mapping = ...\n",
        "# format_prediction = ...\n",
        "\n",
        "def analyze_texts(texts):\n",
        "    \"\"\"对多条文本进行情感分析并按积极程度降序排名\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        # 对输入文本进行分词\n",
        "        input_ids = tf.constant(tokenizer.encode(text, add_special_tokens=True))[None, :]\n",
        "\n",
        "        # 模型预测\n",
        "        preds = model(input_ids)\n",
        "\n",
        "        # 格式化预测结果\n",
        "        formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "        # 提取积极情感的概率值\n",
        "        try:\n",
        "            # 从嵌套结构中获取积极情感概率\n",
        "            positive_probability = formatted_preds[0]['sentiment_probabilities']['positive']\n",
        "        except (IndexError, KeyError, TypeError):\n",
        "            print(f\"警告: 无法解析文本 '{text[:30]}...' 的预测结果\")\n",
        "            positive_probability = 0.0\n",
        "\n",
        "        # 存储结果\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'positive_probability': positive_probability,\n",
        "            'all_predictions': formatted_preds\n",
        "        })\n",
        "\n",
        "    # 按积极概率降序排序\n",
        "    sorted_results = sorted(results, key=lambda x: x['positive_probability'], reverse=True)\n",
        "\n",
        "    return sorted_results\n",
        "\n",
        "# 示例使用\n",
        "sample_texts = [\n",
        "   \"This pandemic situation is really getting better!\",\n",
        "    \"This pandemic situation is really getting worse!\",\n",
        "    \"I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated or not.\",\n",
        "   \"This pandemic situation is really getting better!😍\"\n",
        "]\n",
        "\n",
        "# 分析并排序文本\n",
        "ranked_texts = analyze_texts(sample_texts)\n",
        "\n",
        "# 打印排名结果\n",
        "print(\"\\nTexts ranked by positive sentiment (descending):\")\n",
        "for i, result in enumerate(ranked_texts, 1):\n",
        "    print(f\"\\nRank {i}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Positive Probability: {result['positive_probability']:.4f}\")\n",
        "    print(f\"All Predictions: {json.dumps(result['all_predictions'], indent=4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2ciGb6a_XGQ",
        "outputId": "1a87f7dd-9b63-4e46-cea5-31f0d653e926"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texts ranked by positive sentiment (descending):\n",
            "\n",
            "Rank 1:\n",
            "Text: This pandemic situation is really getting better!\n",
            "Positive Probability: 0.9807\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9806835055351257,\n",
            "            \"negative\": 0.019316459074616432\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 2:\n",
            "Text: This pandemic situation is really getting better!😍\n",
            "Positive Probability: 0.9591\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9590954780578613,\n",
            "            \"negative\": 0.04090452939271927\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 3:\n",
            "Text: I want to get vaccinated.\n",
            "Positive Probability: 0.4858\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.5141684412956238,\n",
            "            \"positive\": 0.4858315885066986\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 4:\n",
            "Text: I am not sure if I want to get vaccinated.\n",
            "Positive Probability: 0.1100\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8899889588356018,\n",
            "            \"positive\": 0.11001105606555939\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 5:\n",
            "Text: I am not sure if I want to get vaccinated or not.\n",
            "Positive Probability: 0.1070\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8929714560508728,\n",
            "            \"positive\": 0.10702849924564362\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 6:\n",
            "Text: This pandemic situation is really getting worse!\n",
            "Positive Probability: 0.0592\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.9407861232757568,\n",
            "            \"positive\": 0.059213921427726746\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su9LMT22_mYi",
        "outputId": "e4b75783-6c92-4839-93be-7cd75888c424"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# 获取版本\n",
        "transformers_version = transformers.__version__\n",
        "tensorflow_version = tf.__version__\n",
        "torch_version = torch.__version__\n",
        "\n",
        "# 保存路径\n",
        "save_path = \"/content/drive/MyDrive/saved_model/\"  # 替换为你的保存路径\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# 写入 requirements.txt\n",
        "with open(os.path.join(save_path, \"requirements.txt\"), \"w\") as f:\n",
        "    f.write(f\"transformers=={transformers_version}\\n\")\n",
        "    f.write(f\"tensorflow=={tensorflow_version}\\n\")\n",
        "    f.write(f\"torch=={torch_version}\\n\")\n",
        "    # 添加其他依赖（如果有）\n",
        "\n",
        "print(f\"版本信息已保存到: {save_path}/requirements.txt\")"
      ],
      "metadata": {
        "id": "NgQk2uHzTJFO",
        "outputId": "bb416407-4397-4f4f-891d-ee34cf0268f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "版本信息已保存到: /content/drive/MyDrive/saved_model//requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "\n",
        "# 保存路径\n",
        "save_path = \"/content/drive/MyDrive/saved_model/\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# 保存模型权重和配置\n",
        "model.save_pretrained(save_path)\n",
        "\n",
        "# 保存分词器（与训练时使用的相同）\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert\")  # 替换为你训练时使用的分词器\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# 保存训练参数（包含所有关键配置）\n",
        "training_args = {\n",
        "    \"model_name\": model_name,  # 使用的预训练模型名称\n",
        "    \"num_labels\": num_labels,  # 分类任务的标签数量\n",
        "    \"max_seq_length\": max_seq_length,  # 最大序列长度\n",
        "    \"train_batch_size\": train_batch_size,  # 训练批量大小\n",
        "    \"eval_batch_size\": eval_batch_size,  # 评估批量大小\n",
        "    \"use_percentage_of_data\": use_percentage_of_data,  # 使用数据的百分比\n",
        "    \"learning_rate\": learning_rate,  # 学习率\n",
        "    \"num_epochs\": num_epochs,  # 训练轮数\n",
        "    \"num_train_examples\": num_train_examples,  # 训练样本总数\n",
        "    \"num_dev_examples\": num_dev_examples,  # 验证样本总数\n",
        "    \"train_steps_per_epoch\": train_steps_per_epoch,  # 每轮训练步数\n",
        "    \"dev_steps_per_epoch\": dev_steps_per_epoch,  # 每轮验证步数\n",
        "    \"optimizer\": optimizer.get_config(),  # 优化器配置\n",
        "    \"loss\": loss.__class__.__name__,  # 损失函数名称\n",
        "    \"metrics\": [m.name for m in metrics],  # 评估指标列表\n",
        "}\n",
        "\n",
        "import pickle\n",
        "\n",
        "# 保存训练参数到 pickle 文件\n",
        "with open(os.path.join(save_path, \"training_args.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(training_args, f)\n",
        "\n",
        "print(f\"训练参数已保存到: {save_path}/training_args.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "iiKtZI5KJ57d",
        "outputId": "ee1acd9d-1e16-4501-e2ab-0589d7344a00"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type float32 is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2001d02db243>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_args.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"训练参数已保存到: {save_path}/training_args.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type float32 is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZh7XAsFVR6h",
        "outputId": "e8317058-6ad1-471a-d4ea-f3b542331abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "训练参数已保存到: /content/drive/MyDrive/saved_model//training_args.pkl\n"
          ]
        }
      ]
    }
  ]
}