{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CT-BERT - Huggingface (GPU training)",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ca90abee2e14f75bb22f5feb0020513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1993815721147e1803f0f491ad93105",
              "IPY_MODEL_ed5774cb508b4218989c8ea572620824",
              "IPY_MODEL_956fb3b4cab14762adf6f168dfe793ee"
            ],
            "layout": "IPY_MODEL_3f94ca1d8b734126896d7371966b6508"
          }
        },
        "d1993815721147e1803f0f491ad93105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a43c21e8459e49c8a9e4e1b16e3cb5b6",
            "placeholder": "​",
            "style": "IPY_MODEL_31c333462d43499c84111dbdf75c4bdc",
            "value": "config.json: 100%"
          }
        },
        "ed5774cb508b4218989c8ea572620824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f60835823b2b44f6bad3d24a58880e98",
            "max": 421,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa7f2cf16d8344cba8caf870476ce5a7",
            "value": 421
          }
        },
        "956fb3b4cab14762adf6f168dfe793ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9a1756ca3a84a0fa2088e1f0a06cfd8",
            "placeholder": "​",
            "style": "IPY_MODEL_0ad3749db46743b5830fefa5bd9339cb",
            "value": " 421/421 [00:00&lt;00:00, 43.2kB/s]"
          }
        },
        "3f94ca1d8b734126896d7371966b6508": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a43c21e8459e49c8a9e4e1b16e3cb5b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c333462d43499c84111dbdf75c4bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f60835823b2b44f6bad3d24a58880e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa7f2cf16d8344cba8caf870476ce5a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9a1756ca3a84a0fa2088e1f0a06cfd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ad3749db46743b5830fefa5bd9339cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d26d41a3681549b1b3e846803d1fc5a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd5e1e0f48b049779263b7d15588223b",
              "IPY_MODEL_534e19033c484759ba5e3a940ccf6bf5",
              "IPY_MODEL_d4648dfe42d443919ac86bc534be6a68"
            ],
            "layout": "IPY_MODEL_31847aaa2dba4b83bd3b4683384f235c"
          }
        },
        "fd5e1e0f48b049779263b7d15588223b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79eb43f00f69415a917f3d3b8bc842fa",
            "placeholder": "​",
            "style": "IPY_MODEL_1ad68da68e1640d89998b85a21edf714",
            "value": "vocab.txt: 100%"
          }
        },
        "534e19033c484759ba5e3a940ccf6bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7fff291e9b94fef8b7ac534e550ce14",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0797ee471d3442a9f559d2a75d0f13b",
            "value": 231508
          }
        },
        "d4648dfe42d443919ac86bc534be6a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f80b0bf74ec4e86b4973286ef2c7f82",
            "placeholder": "​",
            "style": "IPY_MODEL_01832a43ca31442a9d57e97a9d62c651",
            "value": " 232k/232k [00:00&lt;00:00, 2.62MB/s]"
          }
        },
        "31847aaa2dba4b83bd3b4683384f235c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79eb43f00f69415a917f3d3b8bc842fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ad68da68e1640d89998b85a21edf714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7fff291e9b94fef8b7ac534e550ce14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0797ee471d3442a9f559d2a75d0f13b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f80b0bf74ec4e86b4973286ef2c7f82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01832a43ca31442a9d57e97a9d62c651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LydiaTai/covid-bert/blob/main/CT_BERT_Huggingface_(GPU_training)0513.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3gcwjDTd4oY"
      },
      "source": [
        "<img align=\"right\" width=\"450px\" src=\"https://github.com/digitalepidemiologylab/covid-twitter-bert/raw/master/images/COVID-Twitter-BERT-medium.png\">\n",
        "\n",
        "# Finetuning COVID-Twitter-BERT using Huggingface\n",
        "In this notebook we will finetune CT-BERT for sentiment classification using the transformer library by Huggingface.\n",
        "\n",
        "Learn more about this library [here](https://huggingface.co/transformers/).\n",
        "\n",
        "## Before proceeding\n",
        "Create a copy of this notebook by going to \"File - Save a Copy in Drive\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVsmxmSberX2"
      },
      "source": [
        "# Install transformers and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_k-GwPM9JXP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "80a7a970-336e-409b-e7db-da5df62c8b92"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 5.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 15.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=d717d2618d9a205fbddae8ca2ee7be9e7110728eadfbd8b187b3767fe071ad88\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvvPnOFQH2pR"
      },
      "source": [
        "from transformers import (\n",
        "   AutoConfig,\n",
        "   AutoTokenizer,\n",
        "   TFAutoModelForSequenceClassification,\n",
        "   glue_convert_examples_to_features\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrDpLTI7Jt5p"
      },
      "source": [
        "# Choose a Model from the Huggingface Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyLIi4bvImJZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203,
          "referenced_widgets": [
            "8ca90abee2e14f75bb22f5feb0020513",
            "d1993815721147e1803f0f491ad93105",
            "ed5774cb508b4218989c8ea572620824",
            "956fb3b4cab14762adf6f168dfe793ee",
            "3f94ca1d8b734126896d7371966b6508",
            "a43c21e8459e49c8a9e4e1b16e3cb5b6",
            "31c333462d43499c84111dbdf75c4bdc",
            "f60835823b2b44f6bad3d24a58880e98",
            "aa7f2cf16d8344cba8caf870476ce5a7",
            "a9a1756ca3a84a0fa2088e1f0a06cfd8",
            "0ad3749db46743b5830fefa5bd9339cb",
            "d26d41a3681549b1b3e846803d1fc5a0",
            "fd5e1e0f48b049779263b7d15588223b",
            "534e19033c484759ba5e3a940ccf6bf5",
            "d4648dfe42d443919ac86bc534be6a68",
            "31847aaa2dba4b83bd3b4683384f235c",
            "79eb43f00f69415a917f3d3b8bc842fa",
            "1ad68da68e1640d89998b85a21edf714",
            "b7fff291e9b94fef8b7ac534e550ce14",
            "b0797ee471d3442a9f559d2a75d0f13b",
            "2f80b0bf74ec4e86b4973286ef2c7f82",
            "01832a43ca31442a9d57e97a9d62c651"
          ]
        },
        "outputId": "c2ca27cb-62a2-4a03-8286-1ea301abcea9"
      },
      "source": [
        "# Choose model\n",
        "# @markdown >The default model is <i><b>COVID-Twitter-BERT</b></i>. You can however choose <i><b>BERT Base</i></b> or <i><b>BERT Large</i></b> to compare these models to the <i><b>COVID-Twitter-BERT</i></b>. All these three models will be initiated with a random classification layer. If you go directly to the Predict-cell after having compiled the model, you will see that it still runs the predition. However the output will be random. The training steps below will finetune this for the specific task. <br /><br />\n",
        "model_name = 'digitalepidemiologylab/covid-twitter-bert' #@param [\"digitalepidemiologylab/covid-twitter-bert\", \"bert-large-uncased\", \"bert-base-uncased\"]\n",
        "\n",
        "# Initialise tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/421 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ca90abee2e14f75bb22f5feb0020513"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d26d41a3681549b1b3e846803d1fc5a0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdiZDL69JpNC"
      },
      "source": [
        "# Download the SST-2 Dataset and Prepare for Finetuning\n",
        "You can skip this step if you are using the already finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11kI0lhIJoxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe4fd12-69c6-443f-eb2c-45e085e10886"
      },
      "source": [
        "# Parameters\n",
        "#@markdown >Batch size and sequence length needs to be set to prepare the data. The size of the batches depends on available memory. For Colab GPU limit batch size to 8 and sequence length to 96. By reducing the length of the input (max_seq_length) you can also increase the batch size. For a dataset like SST-2 with lots of short sentences. this will likely benefit training.\n",
        "max_seq_length = 96 #@param {type: \"integer\"}\n",
        "train_batch_size =  8#@param {type: \"integer\"}\n",
        "eval_batch_size = 8 #@param {type: \"integer\"}\n",
        "\n",
        "#@markdown >The Glue dataset has around 62000 examples, and we really do not need them all for training a decent model. To cut down training time, please reduse this to only a percentage of the entire set.\n",
        "use_percentage_of_data = 5 #@param {type: \"slider\", min: 1, max: 100}\n",
        "\n",
        "# get dataset sizes\n",
        "glue_builder = tfds.builder('glue/sst2')  # 注意这里改为sst2\n",
        "glue_builder.download_and_prepare() # This line was moved up\n",
        "num_train_examples = glue_builder.info.splits['train'].num_examples\n",
        "num_dev_examples = glue_builder.info.splits['validation'].num_examples\n",
        "num_labels = glue_builder.info.features['label'].num_classes\n",
        "\n",
        "# download datasets\n",
        "glue_builder.download_and_prepare()\n",
        "train_data = glue_builder.as_dataset(split='train')\n",
        "dev_data = glue_builder.as_dataset(split='validation')\n",
        "\n",
        "# 转换为特征的函数\n",
        "def convert_dataset_to_features(dataset, tokenizer, max_length, task, num_examples=None):\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    # 从数据集中提取文本和标签\n",
        "    for example in dataset.take(num_examples or float('inf')):\n",
        "        text = example['sentence'].numpy().decode('utf-8')\n",
        "        label = example['label'].numpy()\n",
        "        texts.append(text)\n",
        "        labels.append(label)\n",
        "\n",
        "    # 使用tokenizer处理文本\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # 创建特征数据集\n",
        "    dataset = tf.data.Dataset.from_tensor_slices({\n",
        "        'input_ids': encoded['input_ids'],\n",
        "        'attention_mask': encoded['attention_mask'],\n",
        "        'labels': tf.convert_to_tensor(labels, dtype=tf.int32)\n",
        "    })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# 计算要使用的数据量\n",
        "num_train_to_use = int(num_train_examples * (use_percentage_of_data/100))\n",
        "num_dev_to_use = int(num_dev_examples * (use_percentage_of_data/100))\n",
        "\n",
        "# 转换数据集为特征\n",
        "train_dataset = convert_dataset_to_features(\n",
        "    train_data, tokenizer, max_length=max_seq_length, task='sst-2', num_examples=num_train_to_use\n",
        ")\n",
        "train_dataset = train_dataset.shuffle(100).batch(train_batch_size)\n",
        "\n",
        "dev_dataset = convert_dataset_to_features(\n",
        "    dev_data, tokenizer, max_length=max_seq_length, task='sst-2', num_examples=num_dev_to_use\n",
        ")\n",
        "dev_dataset = dev_dataset.batch(eval_batch_size)\n",
        "\n",
        "# Map the labels for printing\n",
        "label_mapping = {i: glue_builder.info.features['label'].int2str(i) for i in range(num_labels)}\n",
        "\n",
        "print(f'\\n\\nThe dataset is downloaded. The entire dataset has {num_train_examples + num_dev_examples} examples of which you are using {use_percentage_of_data}%. This will result in a train dataset with {num_train_to_use} examples and a validation dataset with {num_dev_to_use} examples.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The dataset is downloaded. The entire dataset has 68221 examples of which you are using 5%. This will result in a train dataset with 3367 examples and a validation dataset with 43 examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2AsubSgKGDu"
      },
      "source": [
        "# Compile the Model, Train it on the SST-2 Task and Save the Result\n",
        "You can skip this step if you are using the already finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XSbsZFDQTEO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "62f6563d-352e-443a-e602-d5d972c14843"
      },
      "source": [
        "#@markdown >The default learning rate of 2e5 will be fine in most cases\n",
        "learning_rate = 2e-5 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown > Typically these type of models are finetuned for 3 epochs. This can be increased for small datasets and decreased for large datasets.\n",
        "num_epochs = 1  #@param {type: \"integer\"}\n",
        "\n",
        "# Initialise a Model for Sequence Classification with 2 labels\n",
        "config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Metrics and callbacks\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "checkpoint_path = './checkpoints/checkpoint.{epoch:02d}'\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True)]\n",
        "\n",
        "# Compute some variables\n",
        "train_steps_per_epoch = int(num_train_examples * (use_percentage_of_data/100) / train_batch_size)\n",
        "dev_steps_per_epoch = int(num_dev_examples * (use_percentage_of_data/100) / eval_batch_size)\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset,\n",
        "  epochs=num_epochs,\n",
        "  steps_per_epoch=train_steps_per_epoch,\n",
        "  validation_data=dev_dataset,\n",
        "  validation_steps=dev_steps_per_epoch,\n",
        "  callbacks=callbacks)\n",
        "\n",
        "# Print some information about the training\n",
        "print(f'\\nThe training has finished training after {num_epochs} epochs.')\n",
        "print('\\nThe history contains the accuracy and loss at every epoch:')\n",
        "print(json.dumps(history.history, indent=4))\n",
        "\n",
        "print('\\nThe checkpoint callback has generated a checkpoint after every epoch (loss being the training loss, val_loss is the validation loss):')\n",
        "!ls -lha ./checkpoints/\n",
        "\n",
        "print('\\nWe will now save the finetuned model and the corresponding config file on your Colab disk.')\n",
        "model.save_pretrained('./huggingface_model2/')\n",
        "\n",
        "print('\\nTensorflow model and config-file is saved in ./huggingface_model2/')\n",
        "!ls -lha ./huggingface_model2/"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at digitalepidemiologylab/covid-twitter-bert and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9cbb16d6b3ac>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m history = model.fit(train_dataset, \n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_batch_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1802\u001b[0m                         ):\n\u001b[1;32m   1803\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    907\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYamwzWeM9vM"
      },
      "source": [
        "# Predict\n",
        "Let's run some inference with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T0zy0fK9Rmx"
      },
      "source": [
        "# Small function only used for formatting the output\n",
        "def format_prediction(preds, label_mapping, label_name):\n",
        "    preds = tf.nn.softmax(preds, axis=1)\n",
        "    formatted_preds = []\n",
        "    for pred in preds.numpy():\n",
        "        # convert to Python types and sort\n",
        "        pred = {label: float(probability) for label, probability in zip(label_mapping.values(), pred)}\n",
        "        pred = {k: v for k, v in sorted(pred.items(), key=lambda item: item[1], reverse=True)}\n",
        "        formatted_preds.append({label_name: list(pred.keys())[0], f'{label_name}_probabilities': pred})\n",
        "    return formatted_preds"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdk8zasnKJWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb1986e-6096-4f3c-b77f-47e075339d08"
      },
      "source": [
        "#@markdown >Please input text that the model can try to classify\n",
        "input_text = 'I want to get vaccines.'  #@param {type: \"string\"}\n",
        "\n",
        "# Tokenize the input\n",
        "input_ids = tf.constant(tokenizer.encode(input_text, add_special_tokens=True))[None, :]\n",
        "\n",
        "# Run predictions\n",
        "preds = model(input_ids)\n",
        "\n",
        "# format logits\n",
        "formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "print(f'\\nLabel Mapping:{json.dumps(label_mapping, indent=4)}')\n",
        "print(f'\\nLogits: {preds}')\n",
        "print(f'\\nProbabilities:{json.dumps(formatted_preds, indent=4)}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Label Mapping:{\n",
            "    \"0\": \"negative\",\n",
            "    \"1\": \"positive\"\n",
            "}\n",
            "\n",
            "Logits: TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.5320407 ,  0.28646496]], dtype=float32)>, hidden_states=None, attentions=None)\n",
            "\n",
            "Probabilities:[\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.6939190030097961,\n",
            "            \"negative\": 0.3060809373855591\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBq1C2U9ScWg"
      },
      "source": [
        "##### Copyright 2020 Per Egil Kummervold and Martin Müller"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# 假设这些函数和变量已经定义\n",
        "# tokenizer = ...\n",
        "# model = ...\n",
        "# label_mapping = ...\n",
        "# format_prediction = ...\n",
        "\n",
        "def analyze_texts(texts):\n",
        "    \"\"\"对多条文本进行情感分析并按积极程度降序排名\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        # 对输入文本进行分词\n",
        "        input_ids = tf.constant(tokenizer.encode(text, add_special_tokens=True))[None, :]\n",
        "\n",
        "        # 模型预测\n",
        "        preds = model(input_ids)\n",
        "\n",
        "        # 格式化预测结果\n",
        "        formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "        # 提取积极情感的概率值\n",
        "        try:\n",
        "            # 从嵌套结构中获取积极情感概率\n",
        "            positive_probability = formatted_preds[0]['sentiment_probabilities']['positive']\n",
        "        except (IndexError, KeyError, TypeError):\n",
        "            print(f\"警告: 无法解析文本 '{text[:30]}...' 的预测结果\")\n",
        "            positive_probability = 0.0\n",
        "\n",
        "        # 存储结果\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'positive_probability': positive_probability,\n",
        "            'all_predictions': formatted_preds\n",
        "        })\n",
        "\n",
        "    # 按积极概率降序排序\n",
        "    sorted_results = sorted(results, key=lambda x: x['positive_probability'], reverse=True)\n",
        "\n",
        "    return sorted_results\n",
        "\n",
        "# 示例使用\n",
        "sample_texts = [\n",
        "    \"Vaccines are a great achievement for public health!\",\n",
        "    \"I'm not sure about getting vaccinated.\",\n",
        "    \"Getting vaccinated is the best way to protect ourselves.\",\n",
        "    \"Vaccines might have some side effects, but they're generally safe.\"\n",
        "]\n",
        "\n",
        "# 分析并排序文本\n",
        "ranked_texts = analyze_texts(sample_texts)\n",
        "\n",
        "# 打印排名结果\n",
        "print(\"\\nTexts ranked by positive sentiment (descending):\")\n",
        "for i, result in enumerate(ranked_texts, 1):\n",
        "    print(f\"\\nRank {i}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Positive Probability: {result['positive_probability']:.4f}\")\n",
        "    print(f\"All Predictions: {json.dumps(result['all_predictions'], indent=4)}\")"
      ],
      "metadata": {
        "id": "ge4mZcI-9frp",
        "outputId": "5a1beefa-1a67-468f-f1cc-da4df20fef70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texts ranked by positive sentiment (descending):\n",
            "\n",
            "Rank 1:\n",
            "Text: Vaccines are a great achievement for public health!\n",
            "Positive Probability: 0.9934\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9933866858482361,\n",
            "            \"negative\": 0.006613343488425016\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 2:\n",
            "Text: Getting vaccinated is the best way to protect ourselves.\n",
            "Positive Probability: 0.8461\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.8460564017295837,\n",
            "            \"negative\": 0.15394359827041626\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 3:\n",
            "Text: Vaccines might have some side effects, but they're generally safe.\n",
            "Positive Probability: 0.5465\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.5465193390846252,\n",
            "            \"negative\": 0.45348069071769714\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 4:\n",
            "Text: I'm not sure about getting vaccinated.\n",
            "Positive Probability: 0.1696\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8304174542427063,\n",
            "            \"positive\": 0.16958259046077728\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# 假设这些函数和变量已经定义\n",
        "# tokenizer = ...\n",
        "# model = ...\n",
        "# label_mapping = ...\n",
        "# format_prediction = ...\n",
        "\n",
        "def analyze_texts(texts):\n",
        "    \"\"\"对多条文本进行情感分析并按积极程度降序排名\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        # 对输入文本进行分词\n",
        "        input_ids = tf.constant(tokenizer.encode(text, add_special_tokens=True))[None, :]\n",
        "\n",
        "        # 模型预测\n",
        "        preds = model(input_ids)\n",
        "\n",
        "        # 格式化预测结果\n",
        "        formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "        # 提取积极情感的概率值\n",
        "        try:\n",
        "            # 从嵌套结构中获取积极情感概率\n",
        "            positive_probability = formatted_preds[0]['sentiment_probabilities']['positive']\n",
        "        except (IndexError, KeyError, TypeError):\n",
        "            print(f\"警告: 无法解析文本 '{text[:30]}...' 的预测结果\")\n",
        "            positive_probability = 0.0\n",
        "\n",
        "        # 存储结果\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'positive_probability': positive_probability,\n",
        "            'all_predictions': formatted_preds\n",
        "        })\n",
        "\n",
        "    # 按积极概率降序排序\n",
        "    sorted_results = sorted(results, key=lambda x: x['positive_probability'], reverse=True)\n",
        "\n",
        "    return sorted_results\n",
        "\n",
        "# 示例使用\n",
        "sample_texts = [\n",
        "   \"This pandemic situation is really getting better!\",\n",
        "    \"This pandemic situation is really getting worse!\",\n",
        "    \"I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated or not.\",\n",
        "]\n",
        "\n",
        "# 分析并排序文本\n",
        "ranked_texts = analyze_texts(sample_texts)\n",
        "\n",
        "# 打印排名结果\n",
        "print(\"\\nTexts ranked by positive sentiment (descending):\")\n",
        "for i, result in enumerate(ranked_texts, 1):\n",
        "    print(f\"\\nRank {i}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Positive Probability: {result['positive_probability']:.4f}\")\n",
        "    print(f\"All Predictions: {json.dumps(result['all_predictions'], indent=4)}\")"
      ],
      "metadata": {
        "id": "kx6qYks1-_mz",
        "outputId": "b534997e-e507-4d6f-b6e3-d893a39e9051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texts ranked by positive sentiment (descending):\n",
            "\n",
            "Rank 1:\n",
            "Text: This pandemic situation is really getting better!\n",
            "Positive Probability: 0.9807\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9806835055351257,\n",
            "            \"negative\": 0.019316459074616432\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 2:\n",
            "Text: I want to get vaccinated.\n",
            "Positive Probability: 0.4858\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.5141684412956238,\n",
            "            \"positive\": 0.4858315885066986\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 3:\n",
            "Text: I am not sure if I want to get vaccinated.\n",
            "Positive Probability: 0.1100\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8899889588356018,\n",
            "            \"positive\": 0.11001105606555939\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 4:\n",
            "Text: I am not sure if I want to get vaccinated or not.\n",
            "Positive Probability: 0.1070\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8929714560508728,\n",
            "            \"positive\": 0.10702849924564362\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 5:\n",
            "Text: This pandemic situation is really getting worse!\n",
            "Positive Probability: 0.0592\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.9407861232757568,\n",
            "            \"positive\": 0.059213921427726746\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# 假设这些函数和变量已经定义\n",
        "# tokenizer = ...\n",
        "# model = ...\n",
        "# label_mapping = ...\n",
        "# format_prediction = ...\n",
        "\n",
        "def analyze_texts(texts):\n",
        "    \"\"\"对多条文本进行情感分析并按积极程度降序排名\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        # 对输入文本进行分词\n",
        "        input_ids = tf.constant(tokenizer.encode(text, add_special_tokens=True))[None, :]\n",
        "\n",
        "        # 模型预测\n",
        "        preds = model(input_ids)\n",
        "\n",
        "        # 格式化预测结果\n",
        "        formatted_preds = format_prediction(preds[0], label_mapping, 'sentiment')\n",
        "\n",
        "        # 提取积极情感的概率值\n",
        "        try:\n",
        "            # 从嵌套结构中获取积极情感概率\n",
        "            positive_probability = formatted_preds[0]['sentiment_probabilities']['positive']\n",
        "        except (IndexError, KeyError, TypeError):\n",
        "            print(f\"警告: 无法解析文本 '{text[:30]}...' 的预测结果\")\n",
        "            positive_probability = 0.0\n",
        "\n",
        "        # 存储结果\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'positive_probability': positive_probability,\n",
        "            'all_predictions': formatted_preds\n",
        "        })\n",
        "\n",
        "    # 按积极概率降序排序\n",
        "    sorted_results = sorted(results, key=lambda x: x['positive_probability'], reverse=True)\n",
        "\n",
        "    return sorted_results\n",
        "\n",
        "# 示例使用\n",
        "sample_texts = [\n",
        "   \"This pandemic situation is really getting better!\",\n",
        "    \"This pandemic situation is really getting worse!\",\n",
        "    \"I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated.\",\n",
        "    \"I am not sure if I want to get vaccinated or not.\",\n",
        "   \"This pandemic situation is really getting better!😍\"\n",
        "]\n",
        "\n",
        "# 分析并排序文本\n",
        "ranked_texts = analyze_texts(sample_texts)\n",
        "\n",
        "# 打印排名结果\n",
        "print(\"\\nTexts ranked by positive sentiment (descending):\")\n",
        "for i, result in enumerate(ranked_texts, 1):\n",
        "    print(f\"\\nRank {i}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Positive Probability: {result['positive_probability']:.4f}\")\n",
        "    print(f\"All Predictions: {json.dumps(result['all_predictions'], indent=4)}\")"
      ],
      "metadata": {
        "id": "_2ciGb6a_XGQ",
        "outputId": "1a87f7dd-9b63-4e46-cea5-31f0d653e926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texts ranked by positive sentiment (descending):\n",
            "\n",
            "Rank 1:\n",
            "Text: This pandemic situation is really getting better!\n",
            "Positive Probability: 0.9807\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9806835055351257,\n",
            "            \"negative\": 0.019316459074616432\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 2:\n",
            "Text: This pandemic situation is really getting better!😍\n",
            "Positive Probability: 0.9591\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"positive\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"positive\": 0.9590954780578613,\n",
            "            \"negative\": 0.04090452939271927\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 3:\n",
            "Text: I want to get vaccinated.\n",
            "Positive Probability: 0.4858\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.5141684412956238,\n",
            "            \"positive\": 0.4858315885066986\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 4:\n",
            "Text: I am not sure if I want to get vaccinated.\n",
            "Positive Probability: 0.1100\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8899889588356018,\n",
            "            \"positive\": 0.11001105606555939\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 5:\n",
            "Text: I am not sure if I want to get vaccinated or not.\n",
            "Positive Probability: 0.1070\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.8929714560508728,\n",
            "            \"positive\": 0.10702849924564362\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "Rank 6:\n",
            "Text: This pandemic situation is really getting worse!\n",
            "Positive Probability: 0.0592\n",
            "All Predictions: [\n",
            "    {\n",
            "        \"sentiment\": \"negative\",\n",
            "        \"sentiment_probabilities\": {\n",
            "            \"negative\": 0.9407861232757568,\n",
            "            \"positive\": 0.059213921427726746\n",
            "        }\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "su9LMT22_mYi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}